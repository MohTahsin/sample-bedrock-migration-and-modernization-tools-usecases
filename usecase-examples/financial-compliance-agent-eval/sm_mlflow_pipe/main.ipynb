{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfb3c0d",
   "metadata": {},
   "source": [
    "# Financial Compliance Agent Evaluation Pipeline (SageMaker Pipelines + Managed MLflow)\n",
    "\n",
    "This notebook productionalizes the financial compliance agent evaluation flow into a\n",
    "three-step Amazon SageMaker Pipeline:\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   - Load ground-truth dataset from S3  \n",
    "   - Basic validation and dataset profiling  \n",
    "   - Log dataset metadata to **SageMaker managed MLflow**\n",
    "\n",
    "2. **Agent Inference**  \n",
    "   - Build the RAG + Web Search agent (Qwen on Amazon Bedrock)  \n",
    "   - Run inference over all ground-truth prompts  \n",
    "   - Extract normalized outputs (clean answers, retrieved contexts, tool usage)  \n",
    "   - Persist an evaluation-ready dataset to S3  \n",
    "   - Log inference-level metrics to MLflow\n",
    "\n",
    "3. **Evaluation & Metrics**  \n",
    "   - Compute semantic similarity (SAS)  \n",
    "   - Compute tool-selection confusion matrix + accuracy  \n",
    "   - Compute nDCG for RAG retrieval quality  \n",
    "   - (Optionally) run LLM-as-a-judge factuality checks  \n",
    "   - Log all metrics and artifacts to **SageMaker managed MLflow**  \n",
    "   - Enforce an `AccuracyThreshold` quality gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "WARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name moh-user to get Role path.\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::874604298668:role/service-role/AmazonSageMaker-ExecutionRole-20240122T092140e\n",
      "Default bucket: sagemaker-us-east-1-874604298668\n"
     ]
    }
   ],
   "source": [
    "#Cell 2 - Setup\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterInteger,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.function_step import step  # @step decorator\n",
    "\n",
    "\n",
    "# Basic AWS / SageMaker setup\n",
    "session = boto3.Session()\n",
    "region = session.region_name or os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# In SageMaker Studio this will resolve automatically\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    # Fallback for local dev; replace with your role ARN if needed\n",
    "    # role = os.getenv(\"SAGEMAKER_ROLE_ARN\", \"arn:aws:iam::874604298668:role/service-role/YOUR-ROLE-HERE\")\n",
    "    role = os.getenv(\"SAGEMAKER_ROLE_ARN\", \"arn:aws:iam::874604298668:role/service-role/AmazonSageMaker-ExecutionRole-20240122T092140e\")\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "base_job_prefix = \"financial-compliance-agent-eval\"\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Default bucket:\", default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403087a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline parameters created.\n"
     ]
    }
   ],
   "source": [
    "#Cell 3 - Pipeline parameters\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Pipeline parameters (editable when starting the pipeline)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Input ground-truth dataset (ground_truth.json) in S3\n",
    "DataInputS3Uri = ParameterString(\n",
    "    name=\"DataInputS3Uri\",\n",
    "    default_value=f\"s3://{default_bucket}/{base_job_prefix}/data/ground_truth.json\",\n",
    ")\n",
    "\n",
    "# Base output prefix for artifacts & intermediate outputs\n",
    "BaseOutputS3Uri = ParameterString(\n",
    "    name=\"BaseOutputS3Uri\",\n",
    "    default_value=f\"s3://{default_bucket}/{base_job_prefix}/artifacts\",\n",
    ")\n",
    "\n",
    "# SageMaker managed MLflow tracking server ARN\n",
    "MLflowTrackingServerArn = ParameterString(\n",
    "    name=\"MLflowTrackingServerArn\",\n",
    "    #default_value=\"arn:aws:sagemaker:REGION:ACCOUNT_ID:mlflow-tracking-server/YOUR-ID\",\n",
    "    default_value=\"arn:aws:sagemaker:us-east-1:874604298668:mlflow-tracking-server/riv-eval\"\n",
    ")\n",
    "\n",
    "# MLflow experiment name\n",
    "MLflowExperimentName = ParameterString(\n",
    "    name=\"MLflowExperimentName\",\n",
    "    default_value=\"financial-compliance-agent-eval\",\n",
    ")\n",
    "\n",
    "# Bedrock model ID (Qwen or any chat model you want)\n",
    "ModelId = ParameterString(\n",
    "    name=\"ModelId\",\n",
    "    default_value=\"qwen.qwen3-32b-v1:0\",\n",
    ")\n",
    "\n",
    "# Prompt ID (if you later wire Bedrock Prompt Management; not strictly required here)\n",
    "PromptId = ParameterString(\n",
    "    name=\"PromptId\",\n",
    "    default_value=\"financial-compliance-base-prompt\",\n",
    ")\n",
    "\n",
    "# Evaluation + throttling parameters\n",
    "AccuracyThreshold = ParameterFloat(\n",
    "    name=\"AccuracyThreshold\",\n",
    "    default_value=0.8,\n",
    ")\n",
    "\n",
    "RateLimitDelaySeconds = ParameterInteger(\n",
    "    name=\"RateLimitDelaySeconds\",\n",
    "    default_value=10,\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35812a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4 - S3 + MLFlow utilities \n",
    "import io\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _parse_s3_uri(s3_uri: str):\n",
    "    \"\"\"Split s3://bucket/key into (bucket, key).\"\"\"\n",
    "    if not s3_uri.startswith(\"s3://\"):\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "    parsed = urlparse(s3_uri)\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path.lstrip(\"/\")\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def read_json_records_from_s3(s3_uri: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a JSON 'records' file from S3 into a DataFrame.\"\"\"\n",
    "    bucket, key = _parse_s3_uri(s3_uri)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()\n",
    "    return pd.read_json(io.BytesIO(body), orient=\"records\")\n",
    "\n",
    "\n",
    "def write_json_records_to_s3(df: pd.DataFrame, s3_uri: str):\n",
    "    \"\"\"Write a DataFrame as JSON 'records' to S3.\"\"\"\n",
    "    bucket, key = _parse_s3_uri(s3_uri)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    body = df.to_json(orient=\"records\").encode(\"utf-8\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=body)\n",
    "\n",
    "\n",
    "def init_mlflow(tracking_server_arn: str, experiment_name: str):\n",
    "    \"\"\"\n",
    "    Connect to SageMaker managed MLflow and select/create the experiment.\n",
    "\n",
    "    Note: in the console you'll copy the MLflow tracking server ARN and pass\n",
    "    it via `MLflowTrackingServerArn` pipeline parameter.\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5 - Data prep (Step 1)\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@step(\n",
    "    name=\"data-preparation\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")\n",
    "def data_preparation_step(\n",
    "    data_input_s3_uri: str,\n",
    "    base_output_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 1: Load ground-truth dataset, validate, log to MLflow, and\n",
    "    write a normalized dataset to S3 for downstream steps.\n",
    "\n",
    "    Returns:\n",
    "        normalized_dataset_s3_uri (str): S3 URI of normalized dataset.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "\n",
    "    run_name = f\"data-prep-{pipeline_run_id}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        df = read_json_records_from_s3(data_input_s3_uri)\n",
    "\n",
    "        # Basic sanity checks / stats\n",
    "        n_rows = len(df)\n",
    "        n_rag = int((df.get(\"tool_label\") == \"rag\").sum()) if \"tool_label\" in df else 0\n",
    "        n_web = int((df.get(\"tool_label\") == \"web_search\").sum()) if \"tool_label\" in df else 0\n",
    "\n",
    "        mlflow.log_param(\"data_input_s3_uri\", data_input_s3_uri)\n",
    "        mlflow.log_metric(\"num_rows\", n_rows)\n",
    "        mlflow.log_metric(\"num_rag_rows\", n_rag)\n",
    "        mlflow.log_metric(\"num_web_search_rows\", n_web)\n",
    "\n",
    "        # Normalize/ensure expected columns exist\n",
    "        expected_cols = [\"prompt\", \"output\", \"tool_label\", \"context\", \"page\"]\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "        # Output location for normalized dataset\n",
    "        normalized_dataset_s3_uri = (\n",
    "            f\"{base_output_s3_uri.rstrip('/')}/data/ground_truth_normalized.json\"\n",
    "        )\n",
    "\n",
    "        write_json_records_to_s3(df, normalized_dataset_s3_uri)\n",
    "        mlflow.log_param(\"normalized_dataset_s3_uri\", normalized_dataset_s3_uri)\n",
    "\n",
    "    return normalized_dataset_s3_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28cc5ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported fc_agent_eval helpers successfully.\n"
     ]
    }
   ],
   "source": [
    "#Cell 6 - Import helpers \n",
    "# Agent + RAG helpers from fc_agent_eval.py\n",
    "\n",
    "from fc_agent_eval import (\n",
    "    init_chroma_retriever,\n",
    "    build_financial_agent,\n",
    "    format_prompt,\n",
    "    get_clean_docs,\n",
    "    extract_combined_tools,\n",
    ")\n",
    "\n",
    "print(\"Imported fc_agent_eval helpers successfully.\")\n",
    "\n",
    "# Note:\n",
    "# - fc_agent_eval.init_chroma_retriever() uses a DEFAULT_CHROMA_PERSIST_PATH\n",
    "#   that is relative to the repo root: ../data/10k-vec-db\n",
    "# - Make sure you've already built the Chroma DB once (locally or via a\n",
    "#   separate job) so that the persisted index exists at that location.\n",
    "#\n",
    "# If you want the pipeline step to rebuild the index instead, you can\n",
    "# call init_chroma_retriever(...) with pdf_paths + recreate=True inside\n",
    "# the inference step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bebc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 7 - Inference (Step 2)\n",
    "@step(\n",
    "    name=\"agent-inference\",\n",
    "    instance_type=\"ml.g5.xlarge\",   # pick an instance with GPU if needed for Bedrock SDK usage\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")\n",
    "def agent_inference_step(\n",
    "    dataset_s3_uri: str,\n",
    "    model_id: str,\n",
    "    prompt_id: str,  # kept for consistency, even if unused\n",
    "    base_output_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    "    rate_limit_delay: int = 10,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 2: Run agent inference on all prompts, build an evaluation-ready\n",
    "    dataset, and write it to S3.\n",
    "\n",
    "    Returns:\n",
    "        eval_dataset_s3_uri (str): S3 URI of results with columns:\n",
    "          [prompt, output, tool_label, context, page,\n",
    "           clean_answers, extracted_contexts, tool_used, raw_answers]\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "\n",
    "    run_name = f\"inference-{pipeline_run_id}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        df = read_json_records_from_s3(dataset_s3_uri)\n",
    "\n",
    "        mlflow.log_param(\"dataset_s3_uri\", dataset_s3_uri)\n",
    "        mlflow.log_param(\"model_id\", model_id)\n",
    "        mlflow.log_param(\"prompt_id\", prompt_id)\n",
    "        mlflow.log_param(\"rate_limit_delay_seconds\", rate_limit_delay)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Initialize Chroma RAG index (no rebuild by default)\n",
    "        # --------------------------------------------------------------\n",
    "        # If the 10k-vec-db already exists on disk (as in your original\n",
    "        # notebook), this will just attach to it. To rebuild, pass\n",
    "        # pdf_paths=[...], recreate=True.\n",
    "        init_chroma_retriever(\n",
    "            pdf_paths=None,      # or [\"../data/AMZN-2023-10k.pdf\"] if rebuilding\n",
    "            recreate=False,\n",
    "        )\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Build Bedrock-backed financial agent (Qwen + RAG + web_search)\n",
    "        # --------------------------------------------------------------\n",
    "        agent = build_financial_agent(model_id=model_id)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # Run inference over all prompts\n",
    "        # --------------------------------------------------------------\n",
    "        prompts = df[\"prompt\"].tolist()\n",
    "        answers = []\n",
    "\n",
    "        for p in tqdm(prompts, desc=\"Running agent inference\"):\n",
    "            try:\n",
    "                prompt_msg_list = format_prompt(p)  # List[ChatMessage]\n",
    "                res = agent.run(prompt_msg_list)\n",
    "            except Exception as e:\n",
    "                # Keep alignment even if something fails\n",
    "                res = {\"messages\": [], \"error\": str(e)}\n",
    "            answers.append(res)\n",
    "            time.sleep(rate_limit_delay)\n",
    "\n",
    "        # Attach raw answers\n",
    "        df[\"raw_answers\"] = answers\n",
    "\n",
    "        # Clean final answers (mirror your original notebook logic)\n",
    "        def get_final_text(answer):\n",
    "            try:\n",
    "                msgs = answer[\"messages\"]\n",
    "                last = msgs[-1]\n",
    "                # Haystack ChatMessage usually exposes .text\n",
    "                return getattr(last, \"text\", None) or getattr(last, \"content\", None) or \"I don't know\"\n",
    "            except Exception:\n",
    "                return \"I don't know\"\n",
    "\n",
    "        df[\"clean_answers\"] = [get_final_text(a) for a in answers]\n",
    "\n",
    "        # Extract retrieved contexts for RAG (using fc_agent_eval.get_clean_docs)\n",
    "        df[\"extracted_contexts\"] = [get_clean_docs(a) for a in answers]\n",
    "\n",
    "        # Extract tool usage (using fc_agent_eval.extract_combined_tools)\n",
    "        df[\"tool_used\"] = [extract_combined_tools(a) for a in answers]\n",
    "\n",
    "        # Basic counts for logging\n",
    "        tool_counts = df[\"tool_used\"].value_counts().to_dict()\n",
    "        for k, v in tool_counts.items():\n",
    "            mlflow.log_metric(f\"tool_used_{k}\", float(v))\n",
    "\n",
    "        # Write evaluation-ready dataset to S3\n",
    "        eval_dataset_s3_uri = (\n",
    "            f\"{base_output_s3_uri.rstrip('/')}/eval/agent_eval_dataset.json\"\n",
    "        )\n",
    "        write_json_records_to_s3(df, eval_dataset_s3_uri)\n",
    "        mlflow.log_param(\"eval_dataset_s3_uri\", eval_dataset_s3_uri)\n",
    "\n",
    "    return eval_dataset_s3_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f5017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 8 Evaluation & Metrics (Step 3)\n",
    "@step(\n",
    "    name=\"agent-evaluation\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")\n",
    "def agent_evaluation_step(\n",
    "    eval_dataset_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    "    accuracy_threshold: float,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Step 3: Compute evaluation metrics (SAS, tool selection accuracy, nDCG)\n",
    "    and log everything to MLflow.\n",
    "\n",
    "    Returns:\n",
    "        avg_sas_score (float): Average semantic answer similarity.\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    from haystack.components.evaluators import SASEvaluator, DocumentNDCGEvaluator\n",
    "    from haystack import Document\n",
    "\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "    run_name = f\"evaluation-{pipeline_run_id}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        df = read_json_records_from_s3(eval_dataset_s3_uri)\n",
    "        mlflow.log_param(\"eval_dataset_s3_uri\", eval_dataset_s3_uri)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 1) Semantic Answer Similarity (SAS)\n",
    "        # --------------------------------------------------------------\n",
    "        gt = df[\"output\"].tolist()\n",
    "        pa = df[\"clean_answers\"].tolist()\n",
    "\n",
    "        sas_evaluator = SASEvaluator()\n",
    "        sas_evaluator.warm_up()\n",
    "        sas_result = sas_evaluator.run(\n",
    "            ground_truth_answers=gt,\n",
    "            predicted_answers=pa,\n",
    "        )\n",
    "\n",
    "        df[\"sas_score\"] = sas_result[\"individual_scores\"]\n",
    "        avg_sas = float(sas_result[\"score\"])\n",
    "\n",
    "        mlflow.log_metric(\"sas_mean\", avg_sas)\n",
    "        mlflow.log_metric(\"sas_min\", float(np.min(df[\"sas_score\"])))\n",
    "        mlflow.log_metric(\"sas_max\", float(np.max(df[\"sas_score\"])))\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 2) Tool selection confusion matrix & accuracy\n",
    "        # --------------------------------------------------------------\n",
    "        tool_confusion = pd.crosstab(df[\"tool_label\"], df[\"tool_used\"])\n",
    "        correct = 0\n",
    "        for tool in tool_confusion.index:\n",
    "            if tool in tool_confusion.columns:\n",
    "                correct += tool_confusion.loc[tool, tool]\n",
    "        total = tool_confusion.to_numpy().sum()\n",
    "        tool_selection_accuracy = float(correct / total) if total > 0 else 0.0\n",
    "\n",
    "        mlflow.log_metric(\"tool_selection_accuracy\", tool_selection_accuracy)\n",
    "\n",
    "        # Confusion matrix heatmap as artifact\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(\n",
    "            tool_confusion,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            linewidths=0.5,\n",
    "            linecolor=\"gray\",\n",
    "            cbar=True,\n",
    "        )\n",
    "        plt.title(\"Tool Selection Confusion Matrix\")\n",
    "        plt.xlabel(\"Tool Used by Agent\")\n",
    "        plt.ylabel(\"Ground Truth Tool Label\")\n",
    "        plt.tight_layout()\n",
    "        mlflow.log_figure(plt.gcf(), \"tool_selection_confusion_matrix.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 3) RAG retrieval quality (nDCG)\n",
    "        # --------------------------------------------------------------\n",
    "        rag_df = df[df[\"tool_label\"] == \"rag\"].copy()\n",
    "\n",
    "        context_gt = []\n",
    "        for ctx in rag_df[\"context\"].tolist():\n",
    "            if isinstance(ctx, list):\n",
    "                context_gt.append([Document(content=str(doc)) for doc in ctx])\n",
    "            else:\n",
    "                context_gt.append([])\n",
    "\n",
    "        retrieved_docs = rag_df[\"extracted_contexts\"].tolist()\n",
    "\n",
    "        ndcg_evaluator = DocumentNDCGEvaluator()\n",
    "        ndcg_result = ndcg_evaluator.run(\n",
    "            ground_truth_documents=context_gt,\n",
    "            retrieved_documents=retrieved_docs,\n",
    "        )\n",
    "\n",
    "        ndcg_scores = ndcg_result[\"individual_scores\"]\n",
    "        avg_ndcg = float(ndcg_result[\"score\"])\n",
    "\n",
    "        rag_df = rag_df.reset_index(drop=False).rename(columns={\"index\": \"row_index\"})\n",
    "        rag_df[\"ndcg_score\"] = [round(s, 3) for s in ndcg_scores]\n",
    "\n",
    "        mlflow.log_metric(\"ndcg_mean\", avg_ndcg)\n",
    "        mlflow.log_metric(\"ndcg_min\", float(np.min(ndcg_scores)))\n",
    "        mlflow.log_metric(\"ndcg_max\", float(np.max(ndcg_scores)))\n",
    "\n",
    "        # Worst RAG cases CSV artifact\n",
    "        worst_rag = (\n",
    "            rag_df.sort_values(\"ndcg_score\")\n",
    "                 .loc[:, [\"prompt\", \"ndcg_score\"]]\n",
    "                 .head(10)\n",
    "        )\n",
    "        worst_csv = worst_rag.to_csv(index=False)\n",
    "        mlflow.log_text(worst_csv, \"worst_rag_ndcg_cases.csv\")\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 4) Simple quality gate (SAS vs AccuracyThreshold)\n",
    "        # --------------------------------------------------------------\n",
    "        quality_pass = float(avg_sas >= accuracy_threshold)\n",
    "        mlflow.log_metric(\"quality_pass\", quality_pass)\n",
    "        mlflow.log_metric(\"accuracy_threshold\", float(accuracy_threshold))\n",
    "\n",
    "    return avg_sas\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425ecc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline object created: financial-compliance-agent-eval-pipeline\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 - Wire steps\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"financial-compliance-agent-eval-pipeline\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Wire steps together using their DelayedReturn outputs\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Step 1 – Data Prep\n",
    "data_prep = data_preparation_step(\n",
    "    data_input_s3_uri=DataInputS3Uri,\n",
    "    base_output_s3_uri=BaseOutputS3Uri,\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "# Step 2 – Inference (takes the *return value* of step 1 as input)\n",
    "agent_infer = agent_inference_step(\n",
    "    dataset_s3_uri=data_prep,    # <-- pass DelayedReturn directly, no .properties\n",
    "    model_id=ModelId,\n",
    "    prompt_id=PromptId,\n",
    "    base_output_s3_uri=BaseOutputS3Uri,\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    rate_limit_delay=RateLimitDelaySeconds,\n",
    ")\n",
    "\n",
    "# Step 3 – Evaluation (takes the *return value* of step 2 as input)\n",
    "agent_eval = agent_evaluation_step(\n",
    "    eval_dataset_s3_uri=agent_infer,   # <-- pass DelayedReturn directly\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    accuracy_threshold=AccuracyThreshold,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Create SageMaker Pipeline object\n",
    "#   - only the leaf node (agent_eval) is strictly required\n",
    "#   - SageMaker infers upstream dependencies from the DelayedReturn graph\n",
    "# -------------------------------------------------------------------\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        DataInputS3Uri,\n",
    "        BaseOutputS3Uri,\n",
    "        MLflowTrackingServerArn,\n",
    "        MLflowExperimentName,\n",
    "        ModelId,\n",
    "        PromptId,\n",
    "        AccuracyThreshold,\n",
    "        RateLimitDelaySeconds,\n",
    "    ],\n",
    "    steps=[agent_eval],   # leaf node; data_prep & agent_infer inferred automatically\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "print(\"Pipeline object created:\", pipeline_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0437a01",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:861\u001b[39m, in \u001b[36mPath.exists\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:841\u001b[39m, in \u001b[36mPath.stat\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    838\u001b[39m \u001b[33;03mReturn the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[32m    839\u001b[39m \u001b[33;03mos.stat() does.\u001b[39;00m\n\u001b[32m    840\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified: 'c:\\\\.sagemaker-code-config'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Cell 10 - upsert/ run \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Register / update the pipeline definition in SageMaker\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipeline_upsert_response = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole_arn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUpsert response:\u001b[39m\u001b[33m\"\u001b[39m, pipeline_upsert_response)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Example: start an execution (you can also start from the SageMaker console)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtahsin\\Documents\\reInvent-2025\\venv\\Lib\\site-packages\\sagemaker\\workflow\\pipeline.py:292\u001b[39m, in \u001b[36mPipeline.upsert\u001b[39m\u001b[34m(self, role_arn, description, tags, parallelism_config)\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAn AWS IAM role is required to create or update a Pipeline.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole_arn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallelism_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m ce:\n\u001b[32m    294\u001b[39m     error_code = ce.response[\u001b[33m\"\u001b[39m\u001b[33mError\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtahsin\\Documents\\reInvent-2025\\venv\\Lib\\site-packages\\sagemaker\\workflow\\pipeline.py:162\u001b[39m, in \u001b[36mPipeline.create\u001b[39m\u001b[34m(self, role_arn, description, tags, parallelism_config)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sagemaker_session.sagemaker_client.create_pipeline(\u001b[38;5;28mself\u001b[39m, description)\n\u001b[32m    161\u001b[39m tags = format_tags(tags)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m tags = \u001b[43m_append_project_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m tags = \u001b[38;5;28mself\u001b[39m.sagemaker_session._append_sagemaker_config_tags(tags, PIPELINE_TAGS_PATH)\n\u001b[32m    164\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._create_args(role_arn, description, parallelism_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtahsin\\Documents\\reInvent-2025\\venv\\Lib\\site-packages\\sagemaker\\_studio.py:36\u001b[39m, in \u001b[36m_append_project_tags\u001b[39m\u001b[34m(tags, working_dir)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_append_project_tags\u001b[39m(tags=\u001b[38;5;28;01mNone\u001b[39;00m, working_dir=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Appends the project tag to the list of tags, if it exists.\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m \u001b[33;03m        A possibly extended list of tags that includes the project id.\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     path = \u001b[43m_find_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tags\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mtahsin\\Documents\\reInvent-2025\\venv\\Lib\\site-packages\\sagemaker\\_studio.py:70\u001b[39m, in \u001b[36m_find_config\u001b[39m\u001b[34m(working_dir)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wd.match(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     69\u001b[39m     candidate = wd / STUDIO_PROJECT_CONFIG\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     71\u001b[39m         path = candidate\n\u001b[32m     72\u001b[39m     wd = wd.parent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:863\u001b[39m, in \u001b[36mPath.exists\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28mself\u001b[39m.stat(follow_symlinks=follow_symlinks)\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_ignore_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    864\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:52\u001b[39m, in \u001b[36m_ignore_error\u001b[39m\u001b[34m(exception)\u001b[39m\n\u001b[32m     45\u001b[39m _IGNORED_ERRNOS = (ENOENT, ENOTDIR, EBADF, ELOOP)\n\u001b[32m     47\u001b[39m _IGNORED_WINERRORS = (\n\u001b[32m     48\u001b[39m     _WINERROR_NOT_READY,\n\u001b[32m     49\u001b[39m     _WINERROR_INVALID_NAME,\n\u001b[32m     50\u001b[39m     _WINERROR_CANT_RESOLVE_FILENAME)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ignore_error\u001b[39m(exception):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(exception, \u001b[33m'\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m _IGNORED_ERRNOS \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     54\u001b[39m             \u001b[38;5;28mgetattr\u001b[39m(exception, \u001b[33m'\u001b[39m\u001b[33mwinerror\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m _IGNORED_WINERRORS)\n\u001b[32m     57\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cache\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_case_sensitive\u001b[39m(flavour):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Cell 10 - upsert/ run \n",
    "# Register / update the pipeline definition in SageMaker\n",
    "pipeline_upsert_response = pipeline.upsert(role_arn=role)\n",
    "print(\"Upsert response:\", pipeline_upsert_response)\n",
    "\n",
    "# Example: start an execution (you can also start from the SageMaker console)\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"DataInputS3Uri\": f\"s3://{default_bucket}/{base_job_prefix}/data/ground_truth.json\",\n",
    "        \"BaseOutputS3Uri\": f\"s3://{default_bucket}/{base_job_prefix}/artifacts\",\n",
    "        \"MLflowTrackingServerArn\": \"arn:aws:iam::874604298668:role/service-role/AmazonSageMaker-ExecutionRole-20240122T092140e\",\n",
    "        \"MLflowExperimentName\": \"financial-compliance-agent-eval\",\n",
    "        \"ModelId\": \"qwen.qwen3-32b-v1:0\",\n",
    "        \"PromptId\": \"financial-compliance-base-prompt\",\n",
    "        \"AccuracyThreshold\": 0.8,\n",
    "        \"RateLimitDelaySeconds\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Started execution:\", execution.arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
